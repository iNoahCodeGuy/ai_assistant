Question: How does Portfolia handle failures and ensure uptime?

Answer: Portfolia is designed with production-grade resilience principles to ensure conversational continuity even when external services fail. The system follows a "graceful degradation" philosophy: **the conversation should never crash on the user**, even if dependencies like OpenAI, Supabase, email services, or SMS providers become unavailable. This architecture reflects real-world SRE (Site Reliability Engineering) practices where **availability of the core experience (conversation) takes priority over optional features (notifications, analytics)**.

**Error Handling Philosophy**

The system's error handling strategy is built on **five core principles**:

1. **Never Crash on User** - Uncaught exceptions are contained within service boundaries and never propagate to the user interface. All conversation flow nodes return state objects with error flags rather than raising exceptions that would terminate the pipeline.

2. **Graceful Degradation** - When optional services fail (email, SMS, file storage), the system continues operating with reduced functionality. For example, if Resend (email service) is unavailable, the conversation proceeds normally and users simply don't receive email notifications—no error message, no interruption.

3. **Observable Failures** - All errors are logged with full context (traceback, input data, service state) to LangSmith and Supabase analytics tables. This enables post-mortem debugging without exposing technical details to users.

4. **Fail-Fast on Startup** - Critical services (Supabase database, OpenAI API keys) are validated during application initialization. If essential credentials are missing, the app refuses to start with a clear error message, preventing degraded operation.

5. **Defensive Coding** - All external inputs (user queries, email addresses, file uploads) are sanitized against XSS attacks, SQL injection, and path traversal exploits. Length limits prevent memory exhaustion. Malformed inputs return polite error messages, not stack traces.

**Service Layer Resilience**

Portfolia integrates with **four external service categories**: LLM providers (OpenAI), database/storage (Supabase), email (Resend), and SMS (Twilio). Each service uses a **factory pattern** that returns `None` if the service cannot be initialized (missing API keys, network failure, service downtime). This pattern allows the conversation flow to check service availability before attempting operations.

For example, when sending a notification email:
- The system calls `get_resend_service()` which returns the email client or `None`
- If the client exists, it attempts to send the email within a try/except block
- Success: Email sent, user notified, event logged to analytics
- Failure (rate limit, invalid recipient): Error logged with context, conversation continues, user not exposed to error
- Service unavailable: Warning logged, conversation continues without email feature

The same pattern applies to Twilio SMS, Supabase Storage (resume uploads), and analytics logging. **Optional services never block the core conversation**.

**RAG Pipeline Resilience**

The Retrieval-Augmented Generation (RAG) pipeline has **three failure points** with specific handling strategies:

1. **Embedding Generation Failure** (OpenAI API down or rate limited): The retriever catches OpenAI exceptions, logs the error, and returns an empty result set. The LLM then generates a response without retrieved context—essentially falling back to "zero-shot" mode using only the system prompt.

2. **Vector Search Failure** (Supabase timeout or pgvector index corruption): The retriever catches database exceptions, logs the error with query details, and returns an empty chunk list. The system continues with an ungrounded response rather than crashing.

3. **LLM Response Generation Failure** (OpenAI rate limit exceeded): The response generator catches `RateLimitError` specifically and returns a hardcoded fallback message: "I'm experiencing high demand right now. Please try again in a moment." This prevents the cryptic "429 Too Many Requests" error from reaching users.

**Test-backed proof**: The system includes a comprehensive error handling test suite (`tests/test_error_handling.py`) with **5 core tests covering critical failure scenarios**, all passing at 100%:

- `test_conversation_without_twilio` - Verifies conversation continues when SMS service unavailable
- `test_conversation_without_resend` - Verifies conversation continues when email service unavailable
- `test_openai_rate_limit_handling` - Verifies fallback message displays on OpenAI rate limit
- `test_email_validation` - Verifies XSS and SQL injection prevention in email extraction
- `test_invalid_json_in_api` - Verifies JSON parsing errors return structured HTTP 400 responses

These tests use mocking to simulate real-world failure conditions (service returning `None`, OpenAI raising `RateLimitError`) and assert that the system continues operating gracefully.

**API Endpoint Error Handling**

The Vercel serverless API endpoints (`/api/chat`, `/api/email`, `/api/feedback`) follow a **structured error response pattern** with appropriate HTTP status codes:

- **Client errors (400-499)**: Invalid JSON, missing required fields, malformed input → Return `{"success": false, "error": "descriptive message"}` with 400 status. Example: `{"success": false, "error": "Invalid JSON in request body"}` prevents confusion from generic 500 errors.

- **Server errors (500-599)**: Unexpected exceptions, database timeouts, third-party API failures → Return `{"success": false, "error": "Internal server error"}` with 500 status. Detailed exception info (traceback, service state) is logged to LangSmith for debugging but **never exposed to clients** (prevents information disclosure vulnerabilities).

- **Success responses (200)**: Always include `{"success": true, ...}` structure with expected data fields. This consistency allows frontend code to handle responses uniformly.

The API layer uses try/except blocks at **two levels**: (1) specific exception handling for known failure modes (JSON parsing, validation errors), (2) catch-all exception handler that logs full context and returns generic 500 error. This "defense in depth" ensures no exception ever returns a raw Python stack trace to users.

**Conversation Flow Node Resilience**

Portfolia uses a **LangGraph-inspired node-based conversation pipeline** where each node is a pure function that takes a `ConversationState` object and returns an updated state. Nodes **never raise exceptions**—instead, they set error flags in the state and return, allowing subsequent nodes to check for errors and skip processing.

For example, the `retrieve_chunks` node:
- Calls the pgvector retriever and wraps the call in try/except
- Success: Stores chunks in `state.retrieved_chunks`, continues pipeline
- Failure: Sets `state.retrieval_error = True`, logs error, returns state with empty chunks
- Next node (`generate_answer`) checks for `state.retrieval_error` and generates ungrounded response if needed

This pattern means **one failing node doesn't crash the entire pipeline**. The conversation reaches completion even if retrieval, analytics logging, or notification sending fails.

**Input Validation & Security**

All user inputs undergo **sanitization before processing** to prevent common web vulnerabilities:

- **XSS (Cross-Site Scripting)**: HTML tags and JavaScript code in user queries are escaped before storage. Email addresses extracted from queries are validated against a regex pattern that rejects `<script>` tags.

- **SQL Injection**: Supabase Python client uses parameterized queries by default (protection is built-in). Raw SQL is never constructed from user input.

- **Path Traversal**: File upload names are sanitized to remove `../` sequences and null bytes. Uploaded files are stored with UUID-based names, not user-provided names.

- **Length Limits**: User queries are capped at 5,000 characters, email addresses at 320 characters (RFC 5321 maximum), file uploads at 10MB. Exceeding limits returns a polite error: "Your message is too long. Please keep it under 5,000 characters."

**Test-backed proof**: The `test_email_validation` test attempts XSS (`<script>alert('xss')</script>@example.com`) and SQL injection (`test@example.com'; DROP TABLE users; --`) attacks on the email extraction function. The regex validator correctly rejects both attacks by extracting only the valid email portion or returning `None`.

**Production Monitoring & Observability**

Portfolia uses **LangSmith integration** for distributed tracing and error tracking (when `LANGSMITH_API_KEY` is configured):

- **Trace Context**: Every LLM call, retrieval operation, and node execution is traced with input/output/latency
- **Error Aggregation**: Exceptions are grouped by type and frequency (e.g., "10 OpenAI rate limit errors in last hour")
- **Cost Tracking**: Token usage and estimated costs are tracked per session
- **Performance Metrics**: P50/P95/P99 latency percentiles for retrieval and generation

The analytics system (`src/analytics/supabase_analytics.py`) logs all interactions to Supabase tables:
- `messages` table: Full conversation history with session IDs
- `retrieval_logs` table: Which KB chunks were retrieved for each query (enables A/B testing of retrieval strategies)
- `feedback` table: User ratings and contact requests

**Alert thresholds** (Phase 2 implementation):
- Error rate >5% over 1-hour window → Trigger Slack alert to on-call engineer
- Average latency >5 seconds → Investigate database query performance
- OpenAI API cost >$10/day → Review token usage patterns for optimization

**Daily automated reports** (Phase 2 implementation):
- Top 10 most frequent errors with example traces
- Query volume, average latency, error rate, cost trends
- Most retrieved KB chunks (indicates which topics users ask about)

**Known Limitations & Roadmap**

The current error handling implementation is **production-ready for conversational use cases** but has opportunities for enhancement:

**Not yet implemented** (considered for Phase 2):

- **Circuit Breaker Pattern**: After N consecutive failures to a service (e.g., OpenAI), temporarily stop attempting calls and return fallback responses immediately. This prevents wasted API calls and reduces latency during outages. Target: Q1 2026.

- **Exponential Backoff with Jitter**: Retry failed API calls with increasing delays (1s, 2s, 4s, 8s) plus random jitter to avoid "thundering herd" when service recovers. Currently, failures are logged but not retried. Target: Q1 2026.

- **Redis Caching**: Cache LLM responses for common queries (e.g., "What is your tech stack?") to reduce OpenAI API costs and improve latency. Cache invalidation triggered by KB updates. Target: Q2 2026.

- **Health Check Endpoint**: `/api/health` endpoint that returns service status (database, OpenAI, email, SMS) and response time. Used by monitoring tools to detect degradation before users report issues. Target: Q1 2026.

**Future enhancements** (considered for Phase 3):

- **Multi-Region Deployment**: Deploy API to multiple Vercel regions with automatic failover. If US-East region fails, traffic routes to US-West. Requires Supabase multi-region setup. Target: Q2 2026.

- **Request Queuing**: During high traffic, queue excess requests rather than returning 429 errors. Process queue asynchronously with notifications when response ready. Target: Q3 2026.

**Why This Matters for Production Use**

For **technical hiring managers and engineering leadership evaluating this portfolio project**, the error handling architecture demonstrates:

1. **Systems thinking**: Considered failure modes beyond "happy path" (shows maturity)
2. **Observability mindset**: Designed for debuggability from day one (LangSmith tracing, structured logging)
3. **Security awareness**: Input sanitization prevents common vulnerabilities (shows production readiness)
4. **Test-driven quality**: 5 dedicated error handling tests prove the system behaves correctly under failure (not just claims)
5. **Pragmatic tradeoffs**: Acknowledged limitations and roadmap show realistic assessment (not over-engineered)

The system's **99% test pass rate** (76 tests, 75 passing, 1 intentionally skipped) and comprehensive error handling tests prove that **Portfolia is not a toy demo—it's architected like a production SaaS application** with resilience, observability, and user experience prioritized equally with feature functionality.

**Related Questions**: For database architecture and deployment infrastructure, see "What database and vector store does this product use?" For monitoring and observability details, see "How is the system monitored and traced?" (once implemented in Phase 2).
