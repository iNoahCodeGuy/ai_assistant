Question,Answer
"Show me the system architecture diagram","# 🏗️ Complete System Architecture

## High-Level Architecture Diagram

```mermaid
graph TB
    User[👤 User] -->|HTTP Request| Streamlit[Streamlit UI<br/>localhost:8501<br/>src/main.py]
    Streamlit -->|Query + Role| Router[Role Router<br/>src/agents/role_router.py<br/>Classifies query type]
    Router -->|Enhanced Query| RAG[RAG Engine<br/>src/core/rag_engine.py<br/>Orchestrates retrieval]
    
    RAG -->|Generate Embedding| OpenAI_Embed[OpenAI API<br/>text-embedding-3-small<br/>$0.00002/1K tokens]
    OpenAI_Embed -->|1536-dim vector<br/>~200ms| RAG
    
    RAG -->|Vector Search| Retriever[pgvector Retriever<br/>src/retrieval/pgvector_retriever.py<br/>Similarity search]
    Retriever -->|SQL Query<br/>cosine distance| Supabase[(Supabase Postgres<br/>+ pgvector extension<br/>IVFFLAT index)]
    Supabase -->|Top 3 Chunks<br/>similarity > 0.7<br/>~300ms| Retriever
    Retriever -->|Retrieved Context| RAG
    
    RAG -->|Prompt + Context<br/>Role-aware| Generator[Response Generator<br/>src/core/response_generator.py<br/>Builds LLM prompt]
    Generator -->|Chat Completion| OpenAI_LLM[OpenAI API<br/>GPT-4o-mini<br/>$0.15/$0.60 per 1M tokens]
    OpenAI_LLM -->|Generated Answer<br/>~1-2s| Generator
    Generator -->|Response Object| Formatter[Response Formatter<br/>src/agents/response_formatter.py<br/>Markdown formatting]
    
    Formatter -->|Formatted Response| Analytics[Analytics Logger<br/>src/analytics/supabase_analytics.py<br/>Track metrics]
    Analytics -->|INSERT messages<br/>retrieval_logs| Supabase
    Analytics -->|Response + Sources<br/>Display| Streamlit
    
    Supabase -.->|Traces & Metrics| LangSmith[LangSmith<br/>Observability Platform<br/>Cost tracking]
    OpenAI_LLM -.->|API Traces| LangSmith
    
    style Supabase fill:#3ECF8E
    style OpenAI_Embed fill:#10A37F
    style OpenAI_LLM fill:#10A37F
    style LangSmith fill:#1C3C3C
```

## 📁 Core Components Breakdown

### **1. Frontend Layer** (`src/main.py`)
- **Technology**: Streamlit (Python web framework)
- **Current**: localhost:8501
- **Future**: Next.js migration (Phase 3)
- **Features**: Role selection, chat interface, source citations, analytics dashboard
- **Latency**: ~50ms rendering

### **2. Agent Layer** (`src/agents/`)
- **Role Router** (`role_router.py`): Classifies queries and adds role context
- **Response Formatter** (`response_formatter.py`): Markdown formatting, source citations

### **3. Core RAG Engine** (`src/core/`)
- **rag_engine.py**: Main orchestration - retrieval → generation → formatting
- **response_generator.py**: LLM prompt construction and OpenAI API calls
- **Latency**: Total ~2-4 seconds per query

### **4. Retrieval Layer** (`src/retrieval/`)
- **pgvector_retriever.py**: Vector similarity search using pgvector
- **Algorithm**: Cosine similarity with IVFFLAT indexing
- **Performance**: O(√n) complexity vs O(n) sequential scan
- **Threshold**: 0.7 minimum similarity score

### **5. Database Layer** (Supabase)
- **Type**: Managed Postgres with pgvector extension
- **Tables**: kb_chunks, messages, retrieval_logs, links, feedback
- **Indexes**: IVFFLAT on embeddings, B-tree on doc_id/session_id
- **Security**: Row Level Security (RLS) policies

### **6. External Services**
- **OpenAI**: text-embedding-3-small (embeddings) + GPT-4o-mini (chat)
- **LangSmith**: Tracing, cost tracking, error monitoring
- **Resend**: Email notifications (Phase 2)
- **Twilio**: SMS alerts for high-value interactions

## 📊 Performance Metrics

| Component | Latency | Cost | Optimization |
|-----------|---------|------|--------------|
| Embedding Generation | ~200ms | $0.00002/query | Batch processing |
| Vector Search | ~300ms | $0 (included) | IVFFLAT index |
| LLM Generation | ~1-2s | $0.0001/query | Streaming (future) |
| Analytics Logging | ~100ms | $0 (included) | Async writes |
| **Total** | **~2-4s** | **~$0.0001** | Serverless scaling |

## 🔄 Request Flow Summary

1. User submits query with role selection (Hiring Manager/Developer)
2. Role Router enhances query with context
3. RAG Engine generates embedding via OpenAI
4. pgvector searches Supabase for similar chunks
5. Top 3 chunks retrieved (similarity > 0.7)
6. Response Generator builds prompt with context
7. GPT-4o-mini generates role-aware response
8. Response Formatter adds markdown + citations
9. Analytics Logger tracks metrics to Supabase
10. Streamlit displays response with sources

## 🎯 Design Principles

- **Serverless-First**: Stateless functions for Vercel deployment
- **Cost-Optimized**: Batch embeddings, minimize LLM calls
- **Observable**: LangSmith tracing on all external calls
- **Scalable**: pgvector handles millions of chunks efficiently
- **Maintainable**: Modular architecture with clear separation of concerns"
"Show me the data flow diagram","Here's how data flows from user query to response:

```
┌─────────────────────────────────────────────────────────────────┐
│ 1. USER INPUT                                                   │
│ User: ""What tech stack do you use?""                            │
│ Role: Software Developer                                        │
└────────────────┬────────────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ 2. ROLE ROUTER (src/agents/role_router.py)                     │
│ • Adds role context: ""[Software Developer perspective]""        │
│ • Classifies query type: ""technical""                           │
│ • Enhanced query: ""[Dev] What tech stack do you use?""         │
└────────────────┬────────────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ 3. EMBEDDING GENERATION (OpenAI API)                           │
│ • Model: text-embedding-3-small                                 │
│ • Input: Enhanced query string                                  │
│ • Output: [0.023, -0.891, 0.445, ... ] (1536 dimensions)      │
│ • Latency: ~200ms                                               │
└────────────────┬────────────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ 4. VECTOR SIMILARITY SEARCH (Supabase + pgvector)              │
│ • SQL: SELECT * FROM kb_chunks                                  │
│        ORDER BY embedding <=> $query_vector                     │
│        LIMIT 3                                                  │
│ • Returns: Top 3 chunks with similarity scores > 0.7           │
│ • Example: [chunk_42: 0.89, chunk_7: 0.82, chunk_19: 0.76]    │
│ • Latency: ~300ms (IVFFLAT index optimization)                 │
└────────────────┬────────────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ 5. CONTEXT ASSEMBLY (src/core/rag_engine.py)                   │
│ • Formats retrieved chunks with metadata                        │
│ • Adds role-specific instructions                               │
│ • Constructs prompt:                                            │
│   System: ""You are Noah's AI assistant for developers""        │
│   Context: [Retrieved chunks with tech stack info]             │
│   Query: Original user question                                 │
└────────────────┬────────────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ 6. LLM GENERATION (OpenAI GPT-4o-mini)                         │
│ • Model: gpt-4o-mini                                            │
│ • Temperature: 0.7 (balanced creativity)                        │
│ • Max tokens: 500                                               │
│ • Output: Contextual response with technical details           │
│ • Latency: ~1-2s                                                │
└────────────────┬────────────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ 7. RESPONSE FORMATTING (src/agents/response_formatter.py)      │
│ • Adds markdown formatting                                      │
│ • Includes source citations                                     │
│ • Adds expandable sections                                      │
└────────────────┬────────────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ 8. ANALYTICS LOGGING (src/analytics/supabase_analytics.py)     │
│ • Logs to messages table: query, answer, role, latency, tokens │
│ • Logs to retrieval_logs: chunk IDs, similarity scores         │
│ • Tracks: session_id, success rate, grounded flag              │
└────────────────┬────────────────────────────────────────────────┘
                 │
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ 9. UI DISPLAY (Streamlit)                                      │
│ • Shows response with markdown rendering                        │
│ • Expandable ""Sources"" section with retrieved chunks          │
│ • ""System Health"" shows latency and token usage               │
└─────────────────────────────────────────────────────────────────┘

**Total Latency**: ~2-4 seconds (embedding + search + generation + logging)
**Cost**: ~$0.0001 per query (embedding + generation)
```"
"Show me the database schema diagram","Here's the complete Supabase database schema:

```
┌─────────────────────────────────────────────────────────────────┐
│ kb_chunks (Knowledge Base with Vector Embeddings)              │
├─────────────────────────────────────────────────────────────────┤
│ id                BIGSERIAL PRIMARY KEY                         │
│ doc_id            TEXT NOT NULL        ← 'career_kb', 'technical_kb' │
│ section           TEXT NOT NULL        ← Section/question name  │
│ content           TEXT NOT NULL        ← Full Q&A text         │
│ embedding         vector(1536)         ← OpenAI embedding      │
│ metadata          JSONB                ← source, category, etc │
│ created_at        TIMESTAMPTZ          ← Auto-set on insert    │
│ updated_at        TIMESTAMPTZ          ← Auto-updated trigger  │
├─────────────────────────────────────────────────────────────────┤
│ INDEXES:                                                        │
│ • kb_chunks_embedding_idx (IVFFLAT) - Fast cosine similarity   │
│ • kb_chunks_doc_section_idx (B-tree) - Filter by doc/section  │
└─────────────────────────────────────────────────────────────────┘
                 │
                 │ Referenced by
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ messages (Chat Interaction Logs)                               │
├─────────────────────────────────────────────────────────────────┤
│ id                BIGSERIAL PRIMARY KEY                         │
│ session_id        UUID NOT NULL        ← Track conversations   │
│ role_mode         TEXT NOT NULL        ← User's selected role  │
│ query             TEXT NOT NULL        ← User's question       │
│ answer            TEXT NOT NULL        ← Assistant's response  │
│ query_type        TEXT                 ← technical, career, etc│
│ latency_ms        INTEGER              ← Response time        │
│ tokens_prompt     INTEGER              ← OpenAI tokens (input) │
│ tokens_completion INTEGER              ← OpenAI tokens (output)│
│ success           BOOLEAN              ← Query succeeded?      │
│ created_at        TIMESTAMPTZ          ← Timestamp            │
├─────────────────────────────────────────────────────────────────┤
│ INDEXES:                                                        │
│ • messages_session_id_idx - Group by conversation              │
│ • messages_role_mode_idx - Analytics by role                   │
│ • messages_created_at_idx - Time-series queries                │
└─────────────────────────────────────────────────────────────────┘
                 │
                 │ 1:N relationship
                 ▼
┌─────────────────────────────────────────────────────────────────┐
│ retrieval_logs (RAG Pipeline Tracking)                         │
├─────────────────────────────────────────────────────────────────┤
│ id                BIGSERIAL PRIMARY KEY                         │
│ message_id        BIGINT FK → messages(id) ON DELETE CASCADE   │
│ topk_ids          BIGINT[]             ← Array of kb_chunks IDs│
│ scores            FLOAT[]              ← Similarity scores     │
│ grounded          BOOLEAN              ← Response cited sources│
│ created_at        TIMESTAMPTZ                                  │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│ links (External Resources)                                      │
├─────────────────────────────────────────────────────────────────┤
│ key               TEXT PRIMARY KEY     ← 'github', 'linkedin'  │
│ url               TEXT NOT NULL        ← Full URL              │
│ description       TEXT                 ← Human-readable desc   │
│ category          TEXT                 ← 'social', 'media'     │
│ active            BOOLEAN              ← Soft delete flag      │
│ created_at        TIMESTAMPTZ                                  │
│ updated_at        TIMESTAMPTZ          ← Auto-updated trigger  │
└─────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│ feedback (User Ratings & Contact Requests)                     │
├─────────────────────────────────────────────────────────────────┤
│ id                BIGSERIAL PRIMARY KEY                         │
│ message_id        BIGINT FK → messages(id) ON DELETE SET NULL  │
│ rating            INTEGER CHECK (1-5)  ← Star rating           │
│ comment           TEXT                 ← Optional feedback     │
│ contact_requested BOOLEAN              ← User wants followup   │
│ email             TEXT                 ← Contact email         │
│ notification_sent BOOLEAN              ← Twilio SMS sent?      │
│ created_at        TIMESTAMPTZ                                  │
└─────────────────────────────────────────────────────────────────┘

**Key Features:**
• **pgvector Extension**: Enables vector similarity search (cosine distance)
• **Row Level Security (RLS)**: service_role full access, authenticated read/write
• **IVFFLAT Index**: ~100x faster than sequential scan for vector search
• **Foreign Keys**: Cascade deletes maintain referential integrity
• **Triggers**: Auto-update updated_at on kb_chunks and links
```"
"Show me a code example of the RAG retrieval","# 🔍 Complete RAG Retrieval Implementation

## Core Retrieval Code

```python
# src/retrieval/pgvector_retriever.py

from typing import List, Dict, Any, Optional
from openai import OpenAI
from config.supabase_config import get_supabase_client, supabase_settings
import logging

logger = logging.getLogger(__name__)

class PgvectorRetriever:
    \"\"\"
    Retrieves relevant knowledge base chunks using pgvector similarity search.
    
    Features:
    - OpenAI text-embedding-3-small for 1536-dim embeddings
    - Cosine similarity search with pgvector
    - IVFFLAT index for O(√n) performance
    - Configurable threshold and top-k filtering
    - Doc-type filtering support
    \"\"\"
    
    def __init__(self):
        self.openai_client = OpenAI(api_key=supabase_settings.api_key)
        self.supabase = get_supabase_client()
        self.embedding_model = 'text-embedding-3-small'
        self.embedding_dim = 1536
    
    def retrieve(
        self, 
        query: str, 
        top_k: int = 3, 
        threshold: float = 0.7,
        doc_filter: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        \"\"\"
        Main retrieval pipeline:
        1. Generate query embedding with OpenAI
        2. Search similar vectors in Supabase using pgvector
        3. Filter by similarity threshold
        4. Optionally filter by doc_id
        5. Return top K results with metadata
        
        Args:
            query: User's search query
            top_k: Number of results to return (default: 3)
            threshold: Minimum similarity score 0-1 (default: 0.7)
            doc_filter: List of doc_ids to search (e.g., ['technical_kb', 'career_kb'])
        
        Returns:
            List of dicts with id, content, similarity, metadata
        \"\"\"
        
        # Step 1: Generate embedding for query
        logger.info(f\"Generating embedding for query: {query[:50]}...\")
        embedding = self._generate_embedding(query)
        logger.info(f\"Embedding generated: {len(embedding)} dimensions\")
        
        # Step 2: Vector similarity search using pgvector
        # Uses custom Postgres function with cosine distance operator: <=>
        logger.info(f\"Searching Supabase with threshold={threshold}, top_k={top_k}\")
        
        params = {
            'query_embedding': embedding,
            'match_threshold': threshold,
            'match_count': top_k
        }
        
        # Add doc_filter if specified
        if doc_filter:
            params['doc_filter'] = doc_filter
            results = self.supabase.rpc('match_kb_chunks_filtered', params).execute()
        else:
            results = self.supabase.rpc('match_kb_chunks', params).execute()
        
        # Step 3: Format results with full metadata
        chunks = []
        for row in results.data:
            chunks.append({
                'id': row['id'],
                'doc_id': row['doc_id'],
                'section': row['section'],
                'content': row['content'],
                'similarity': round(row['similarity'], 4),
                'metadata': row['metadata']
            })
        
        logger.info(f\"Retrieved {len(chunks)} chunks above threshold\")
        return chunks
    
    def _generate_embedding(self, text: str) -> List[float]:
        \"\"\"
        Generate 1536-dimensional embedding using OpenAI.
        
        Cost: ~$0.00002 per 1K tokens
        Latency: ~200ms
        \"\"\"
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text,
                encoding_format='float'  # Returns standard floats
            )
            return response.data[0].embedding
        
        except Exception as e:
            logger.error(f\"Embedding generation failed: {e}\")
            raise
    
    def batch_embed(self, texts: List[str]) -> List[List[float]]:
        \"\"\"
        Generate embeddings for multiple texts in a single API call.
        More efficient than individual calls.
        
        Max: 100 texts per batch
        \"\"\"
        if len(texts) > 100:
            raise ValueError(\"Maximum 100 texts per batch\")
        
        response = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=texts
        )
        
        return [item.embedding for item in response.data]


# === Postgres Functions (in Supabase SQL Editor) ===

# Function 1: Basic similarity search
CREATE OR REPLACE FUNCTION match_kb_chunks(
    query_embedding vector(1536),
    match_threshold float DEFAULT 0.7,
    match_count int DEFAULT 3
)
RETURNS TABLE (
    id bigint,
    doc_id text,
    section text,
    content text,
    similarity float,
    metadata jsonb
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        kb_chunks.id,
        kb_chunks.doc_id,
        kb_chunks.section,
        kb_chunks.content,
        -- Convert cosine distance to similarity (0-1, higher is better)
        1 - (kb_chunks.embedding <=> query_embedding) AS similarity,
        kb_chunks.metadata
    FROM kb_chunks
    WHERE 1 - (kb_chunks.embedding <=> query_embedding) > match_threshold
    ORDER BY kb_chunks.embedding <=> query_embedding  -- Cosine distance (lower is better)
    LIMIT match_count;
END;
$$;

# Function 2: With doc_id filtering
CREATE OR REPLACE FUNCTION match_kb_chunks_filtered(
    query_embedding vector(1536),
    match_threshold float DEFAULT 0.7,
    match_count int DEFAULT 3,
    doc_filter text[] DEFAULT NULL
)
RETURNS TABLE (
    id bigint,
    doc_id text,
    section text,
    content text,
    similarity float,
    metadata jsonb
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        kb_chunks.id,
        kb_chunks.doc_id,
        kb_chunks.section,
        kb_chunks.content,
        1 - (kb_chunks.embedding <=> query_embedding) AS similarity,
        kb_chunks.metadata
    FROM kb_chunks
    WHERE 
        1 - (kb_chunks.embedding <=> query_embedding) > match_threshold
        AND (doc_filter IS NULL OR kb_chunks.doc_id = ANY(doc_filter))
    ORDER BY kb_chunks.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;

# === Index Creation for Performance ===

-- IVFFLAT index: O(√n) vs O(n) sequential scan
CREATE INDEX IF NOT EXISTS kb_chunks_embedding_idx 
ON kb_chunks 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);  -- For ~10K chunks, use lists = 100

-- Composite index for filtered searches
CREATE INDEX IF NOT EXISTS kb_chunks_doc_section_idx 
ON kb_chunks (doc_id, section);
```

## 📊 Performance Analysis

### **Similarity Search Complexity**
- **Without Index**: O(n) - sequential scan, ~1000ms for 10K chunks
- **With IVFFLAT**: O(√n) - approximate search, ~300ms for 10K chunks
- **Trade-off**: 99.5% accuracy with 3x faster search

### **Embedding Costs**
- **Single query**: ~50 tokens = $0.000001
- **1K queries/day**: ~50K tokens = $0.001/day = $0.30/month
- **Caching**: Repeated queries use same embedding (future optimization)

### **Key Operators**
- **<=>**: Cosine distance (0 = identical, 2 = opposite)
- **1 - distance**: Similarity score (0 = different, 1 = identical)
- **threshold > 0.7**: Only high-quality matches returned

## 🎯 Usage Examples

```python
# Example 1: Basic retrieval
retriever = PgvectorRetriever()
results = retriever.retrieve(
    query=\"What's the tech stack?\",
    top_k=3,
    threshold=0.7
)

for chunk in results:
    print(f\"Similarity: {chunk['similarity']:.2f}\")
    print(f\"Content: {chunk['content'][:100]}...\")

# Example 2: Filter by doc type
results = retriever.retrieve(
    query=\"Show me code examples\",
    top_k=5,
    threshold=0.65,
    doc_filter=['technical_kb', 'architecture_kb']
)

# Example 3: Batch embedding generation
texts = [
    \"Question 1 content\",
    \"Question 2 content\",
    \"Question 3 content\"
]
embeddings = retriever.batch_embed(texts)
# Returns 3 embeddings in single API call
```

## 🔍 How It Works

1. **Query Embedding**: Convert text to 1536-dim vector using OpenAI
2. **Vector Search**: Use pgvector's <=> operator for cosine distance
3. **IVFFLAT Index**: Approximate nearest neighbor search (99.5% accurate)
4. **Threshold Filter**: Only return chunks with similarity > 0.7
5. **Top-K Selection**: Return best 3 matches to avoid context overflow
6. **Metadata Return**: Include doc_id, section, similarity for citations"
"Show me how the response generation works","Here's the response generation implementation:

```python
# src/core/response_generator.py

from openai import OpenAI
from typing import List, Dict, Any
from config.supabase_config import supabase_settings

class ResponseGenerator:
    """"""Generates contextual responses using retrieved knowledge and LLM.""""""
    
    def __init__(self):
        self.client = OpenAI(api_key=supabase_settings.api_key)
    
    def generate(
        self,
        query: str,
        retrieved_chunks: List[Dict[str, Any]],
        role: str = ""General""
    ) -> Dict[str, Any]:
        """"""
        Generate response with:
        1. Role-specific system prompt
        2. Retrieved context from knowledge base
        3. User's original query
        4. LLM generation with GPT-4o-mini
        """"""
        
        # Step 1: Build role-specific system prompt
        system_prompt = self._get_system_prompt(role)
        
        # Step 2: Format retrieved chunks as context
        context = self._format_context(retrieved_chunks)
        
        # Step 3: Construct full prompt
        messages = [
            {""role"": ""system"", ""content"": system_prompt},
            {""role"": ""user"", ""content"": f""Context:\\n{context}\\n\\nQuestion: {query}""}
        ]
        
        # Step 4: Call OpenAI GPT-4o-mini
        response = self.client.chat.completions.create(
            model=""gpt-4o-mini"",
            messages=messages,
            temperature=0.7,  # Balance between creativity and accuracy
            max_tokens=500,   # Limit response length
            presence_penalty=0.1,  # Slight preference for new topics
            frequency_penalty=0.1  # Slight penalty for repetition
        )
        
        # Step 5: Extract and return results
        return {
            ""answer"": response.choices[0].message.content,
            ""tokens_prompt"": response.usage.prompt_tokens,
            ""tokens_completion"": response.usage.completion_tokens,
            ""model"": response.model,
            ""finish_reason"": response.choices[0].finish_reason,
            ""retrieved_chunks"": retrieved_chunks
        }
    
    def _get_system_prompt(self, role: str) -> str:
        """"""Returns role-specific instructions for the LLM.""""""
        prompts = {
            ""Software Developer"": 
                ""You are Noah's AI assistant speaking to a software developer. ""
                ""Provide technical details, code examples, and architecture explanations. ""
                ""Focus on implementation details, tech stack, and development practices."",
            
            ""Hiring Manager"":
                ""You are Noah's AI assistant speaking to a hiring manager. ""
                ""Emphasize career achievements, impact metrics, and business value. ""
                ""Highlight transferable skills and project outcomes."",
            
            ""General"":
                ""You are Noah's AI assistant. Provide helpful, accurate responses ""
                ""based on the knowledge base. Be professional and concise.""
        }
        return prompts.get(role, prompts[""General""])
    
    def _format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """"""Formats retrieved chunks into readable context.""""""
        if not chunks:
            return ""No relevant information found in knowledge base.""
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            context_parts.append(
                f""Source {i} (similarity: {chunk['similarity']:.2f}):\\n""
                f""{chunk['content']}\\n""
            )
        
        return ""\\n"".join(context_parts)


# Example usage in RAG pipeline:
retriever = PgvectorRetriever()
generator = ResponseGenerator()

# User query
query = ""What's the tech stack?""
role = ""Software Developer""

# Retrieve relevant chunks
chunks = retriever.retrieve(query, top_k=3)

# Generate response
result = generator.generate(query, chunks, role)

print(result[""answer""])  # Contextual response
print(f""Tokens used: {result['tokens_prompt']} + {result['tokens_completion']}"")
```

**Flow:**
1. **System Prompt**: Sets role-specific instructions
2. **Context Assembly**: Formats retrieved chunks with similarity scores
3. **LLM Call**: GPT-4o-mini generates response with context
4. **Token Tracking**: Logs usage for cost monitoring
5. **Response**: Returns answer + metadata for analytics"
"Show me the role routing logic","Here's how role-based routing works:

```python
# src/agents/role_router.py

from typing import Dict, Tuple
import re

class RoleRouter:
    """"""Routes queries based on user role and query type for optimized retrieval.""""""
    
    ROLE_PROMPTS = {
        ""Software Developer"": {
            ""focus"": ""technical"",
            ""keywords"": [""code"", ""architecture"", ""stack"", ""api"", ""database"", ""implementation""],
            ""context"": ""Focus on technical depth, code examples, and system design""
        },
        ""Hiring Manager"": {
            ""focus"": ""career"",
            ""keywords"": [""experience"", ""achieved"", ""impact"", ""led"", ""delivered"", ""metrics""],
            ""context"": ""Emphasize achievements, leadership, and business impact""
        },
        ""Product Manager"": {
            ""focus"": ""product"",
            ""keywords"": [""features"", ""users"", ""roadmap"", ""requirements"", ""stakeholders""],
            ""context"": ""Highlight product thinking and stakeholder management""
        }
    }
    
    def route(self, query: str, role: str) -> Dict[str, any]:
        """"""
        Process query with role context:
        1. Classify query type (technical, career, personal)
        2. Add role-specific context
        3. Adjust retrieval parameters
        4. Return enhanced query with metadata
        """"""
        
        # Step 1: Classify query type
        query_type = self._classify_query(query)
        
        # Step 2: Get role configuration
        role_config = self.ROLE_PROMPTS.get(role, {})
        
        # Step 3: Enhance query with role context
        enhanced_query = self._enhance_query(query, role, role_config)
        
        # Step 4: Determine retrieval strategy
        retrieval_params = self._get_retrieval_params(query_type, role)
        
        return {
            ""original_query"": query,
            ""enhanced_query"": enhanced_query,
            ""query_type"": query_type,
            ""role"": role,
            ""retrieval_params"": retrieval_params,
            ""role_context"": role_config.get(""context"", """")
        }
    
    def _classify_query(self, query: str) -> str:
        """"""Classify query into type: technical, career, personal, product.""""""
        query_lower = query.lower()
        
        # Technical indicators
        if any(word in query_lower for word in [
            ""code"", ""implement"", ""architecture"", ""database"", ""api"",
            ""tech stack"", ""algorithm"", ""system"", ""deploy"", ""how does""
        ]):
            return ""technical""
        
        # Career indicators
        if any(word in query_lower for word in [
            ""experience"", ""career"", ""worked on"", ""achieved"", ""role"",
            ""transition"", ""why"", ""background"", ""skills"", ""project""
        ]):
            return ""career""
        
        # Product indicators
        if any(word in query_lower for word in [
            ""feature"", ""product"", ""user"", ""roadmap"", ""planning"",
            ""requirements"", ""stakeholder"", ""metrics"", ""impact""
        ]):
            return ""product""
        
        # Default
        return ""general""
    
    def _enhance_query(self, query: str, role: str, role_config: Dict) -> str:
        """"""Add role context to query for better retrieval.""""""
        context = role_config.get(""context"", """")
        
        # For developers, emphasize technical depth
        if role == ""Software Developer"":
            return f""[Technical perspective needed] {query} [Include implementation details]""
        
        # For hiring managers, emphasize achievements
        elif role == ""Hiring Manager"":
            return f""[Career achievement focus] {query} [Highlight impact and results]""
        
        # Default: minimal enhancement
        return query
    
    def _get_retrieval_params(self, query_type: str, role: str) -> Dict:
        """"""Determine optimal retrieval parameters based on query and role.""""""
        params = {
            ""top_k"": 3,
            ""threshold"": 0.7,
            ""doc_filter"": None  # Filter by doc_id if needed
        }
        
        # Technical queries from developers: prioritize technical_kb
        if query_type == ""technical"" and role == ""Software Developer"":
            params[""top_k""] = 5  # More context for technical questions
            params[""doc_filter""] = [""technical_kb"", ""career_kb""]
        
        # Career queries from hiring managers: prioritize career_kb
        elif query_type == ""career"" and role == ""Hiring Manager"":
            params[""doc_filter""] = [""career_kb""]
            params[""threshold""] = 0.65  # Slightly lower threshold for broader matches
        
        return params


# Example usage:
router = RoleRouter()
result = router.route(
    query=""What's the database architecture?"",
    role=""Software Developer""
)

print(result[""query_type""])  # ""technical""
print(result[""enhanced_query""])  # Enhanced with technical context
print(result[""retrieval_params""])  # {""top_k"": 5, ""doc_filter"": [...]}
```

**Smart Routing:**
- **Query Classification**: Detects technical vs career vs product questions
- **Role Enhancement**: Adds context based on user role
- **Dynamic Parameters**: Adjusts retrieval strategy (top_k, threshold, filters)
- **Doc Filtering**: Can prioritize technical_kb or career_kb based on role"
"Show me the complete file structure","# 📂 Complete Codebase File Structure

## Project Tree

```
noahs-ai-assistant/
│
├── 📁 src/                          # Main source code
│   ├── 📁 core/                     # Core RAG engine
│   │   ├── rag_engine.py           # Main orchestration (300 lines)
│   │   ├── response_generator.py   # LLM prompt construction (200 lines)
│   │   └── __init__.py
│   │
│   ├── 📁 agents/                   # Agent layer for routing & formatting
│   │   ├── role_router.py          # Query classification & role context (250 lines)
│   │   ├── response_formatter.py   # Markdown formatting & citations (180 lines)
│   │   └── __init__.py
│   │
│   ├── 📁 retrieval/                # Vector similarity search
│   │   ├── pgvector_retriever.py   # Supabase pgvector integration (220 lines)
│   │   ├── __init__.py
│   │   └── [DEPRECATED] faiss_retriever.py  # Old FAISS implementation
│   │
│   ├── 📁 analytics/                # Metrics & logging
│   │   ├── supabase_analytics.py   # Logs to messages/retrieval_logs (280 lines)
│   │   ├── comprehensive_analytics.py  # Dashboard queries (400 lines)
│   │   ├── data_management.py      # Export & cleanup utilities (350 lines)
│   │   └── __init__.py
│   │
│   ├── 📁 config/                   # Configuration management
│   │   ├── supabase_config.py      # Supabase client & settings (120 lines)
│   │   ├── __init__.py
│   │   └── [DEPRECATED] settings.py
│   │
│   └── main.py                      # Streamlit app entry point (350 lines)
│
├── 📁 data/                         # Knowledge base CSV files
│   ├── career_kb.csv               # Noah's career info (20 Q&A pairs)
│   ├── technical_kb.csv            # Tech stack & features (13 Q&A pairs)
│   ├── architecture_kb.csv         # Diagrams & code examples (6 items) ⭐
│   ├── mma_kb.csv                  # Personal interest knowledge (optional)
│   └── code_chunks/                # Code snippet storage (future)
│
├── 📁 supabase/                     # Database migrations
│   └── migrations/
│       ├── CLEAN_MIGRATION.sql     # Idempotent schema setup (200 lines)
│       ├── SIMPLE_MIGRATION.sql    # Basic schema (180 lines)
│       └── archive/sql/fix_search_function.sql # pgvector search optimization
│
├── 📁 tests/                        # Unit & integration tests
│   ├── test_role_functionality.py  # Role-based response testing (546 lines)
│   ├── test_roles_quick.py         # Quick smoke tests (174 lines)
│   ├── test_connection.py          # API key validation
│   ├── test_retriever_fixed.py     # pgvector search tests
│   └── conftest.py                 # Pytest fixtures
│
├── 📁 scripts/                      # Utility scripts
│   ├── migrate_data_to_supabase.py # Batch CSV → Supabase (150 lines)
│   ├── add_technical_kb.py         # Add technical knowledge (100 lines)
│   ├── add_architecture_kb.py      # Add architecture diagrams (100 lines) ⭐
│   ├── daily_maintenance.py        # Automated cleanup & backups (200 lines)
│   └── verify_schema.py            # Database health check
│
├── 📁 docs/                         # Documentation
│   ├── ARCHITECTURE.md             # System design overview
│   ├── ROLE_FEATURES.md            # Role-based features guide
│   ├── API_KEY_SETUP.md            # Setup instructions
│   └── archive/                    # Historical docs
│
├── 📁 backups/                      # Database backups
│   └── analytics_backup_*.db.gz    # Gzipped SQLite exports
│
├── 📁 vector_stores/                # [DEPRECATED] FAISS indexes
│   └── [Replaced by Supabase pgvector]
│
├── 📁 examples/                     # Demo scripts
│   ├── demo_common_questions.py    # Test common queries
│   ├── demo_data_management.py     # Analytics demo
│   └── example_streamlit_integration.py
│
├── .env                             # Environment variables (gitignored)
├── .env.example                     # Template for API keys
├── requirements.txt                 # Python dependencies (25 packages)
├── README.md                        # Project overview
└── .gitignore                       # Git exclusions

```

## 📋 Key Files Explained

### **Entry Points**
- **src/main.py**: Streamlit app - UI, role selection, chat interface, analytics dashboard

### **Core RAG Pipeline**
1. **src/core/rag_engine.py**: Orchestrates retrieval → generation → formatting
2. **src/retrieval/pgvector_retriever.py**: Vector search with Supabase
3. **src/core/response_generator.py**: LLM prompt construction & OpenAI calls

### **Agent Layer**
- **src/agents/role_router.py**: Classifies queries, adds role context
- **src/agents/response_formatter.py**: Markdown formatting, source citations

### **Analytics & Monitoring**
- **src/analytics/supabase_analytics.py**: Logs every query to Supabase
- **src/analytics/comprehensive_analytics.py**: Dashboard queries (7-day metrics, success rate, etc.)

### **Configuration**
- **src/config/supabase_config.py**: Supabase client singleton, API key management

### **Data Migration**
- **scripts/migrate_data_to_supabase.py**: Batch CSV → Supabase with embeddings
- **add_technical_kb.py / add_architecture_kb.py**: Add specific knowledge domains

### **Testing**
- **tests/test_role_functionality.py**: Comprehensive role-based testing
- **tests/test_connection.py**: Pre-flight API key validation

## 📊 Lines of Code by Component

| Component | Lines | Purpose |
|-----------|-------|---------|
| Core RAG Engine | ~700 | Retrieval, generation, orchestration |
| Agents | ~430 | Routing, formatting, role logic |
| Analytics | ~1030 | Logging, metrics, dashboards |
| Main App | ~350 | Streamlit UI |
| Config | ~120 | Settings management |
| Tests | ~1200 | Quality assurance |
| Scripts | ~650 | Data migration, maintenance |
| **Total** | **~4,480** | Production Python code |

## 🔗 Dependency Flow

```
main.py (Streamlit)
    ↓
role_router.py (classify query)
    ↓
rag_engine.py (orchestrate)
    ↓
pgvector_retriever.py (search) → supabase_config.py → Supabase
    ↓
response_generator.py (generate) → OpenAI API
    ↓
response_formatter.py (format)
    ↓
supabase_analytics.py (log) → Supabase
    ↓
main.py (display)
```

## 🎯 Adding New Features

### **To add a new knowledge domain:**
1. Create `data/new_domain_kb.csv` with Question/Answer format
2. Create `add_new_domain_kb.py` (copy from add_technical_kb.py)
3. Run script to generate embeddings and insert
4. Knowledge immediately searchable

### **To modify retrieval logic:**
1. Edit `src/retrieval/pgvector_retriever.py`
2. Adjust threshold, top_k, or add filters
3. Test with `tests/test_retriever_fixed.py`

### **To add a new role:**
1. Edit `src/agents/role_router.py` - add to ROLE_PROMPTS dict
2. Edit `src/main.py` - add role to sidebar selection
3. Test with `tests/test_role_functionality.py`

## 🚀 Future Additions

- **src/document_processors/**: Resume/cover letter parsing (Phase 4)
- **src/voice/**: Speech-to-text interface (Phase 5)
- **frontend/**: Next.js app to replace Streamlit (Phase 3)
- **api/**: REST API endpoints for external integrations"
