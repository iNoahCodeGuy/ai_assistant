Question,Answer
What is the tech stack for this AI assistant product?,"This AI assistant is built on a modern, scalable tech stack: **Backend/Core**: Python 3.11+ with OpenAI GPT-4o-mini for language generation and text-embedding-3-small for vector embeddings (1536 dimensions). **Database**: Supabase Postgres with pgvector extension for vector similarity search, replacing the previous FAISS implementation. Uses IVFFLAT indexes for fast semantic search. **RAG System**: Custom Retrieval-Augmented Generation engine using LangChain and LangSmith for observability. Implements hybrid retrieval with role-specific context filtering. **External Services**: Resend for email delivery, Twilio for SMS notifications, Supabase Storage for file management. **Frontend**: Streamlit for the current UI (Python-based), with plans for Next.js migration in Phase 3. **Deployment**: Designed for Vercel serverless functions with Supabase backend. **Monitoring**: LangSmith integration for tracing, analytics tracking in Supabase. **Development**: Git/GitHub for version control, Python virtual environments, GitHub Copilot assistance."
What database and vector store does this product use?,"The product uses **Supabase Postgres with pgvector extension** as both the relational database and vector store. This replaced the previous FAISS local vector store in the migration completed October 2025. The database includes 5 main tables: (1) kb_chunks - stores knowledge base content with 1536-dimensional vector embeddings for semantic search, (2) messages - logs all chat interactions with session tracking, (3) retrieval_logs - tracks which KB chunks were retrieved for each query (RAG pipeline monitoring), (4) links - external resources (GitHub, LinkedIn, YouTube), (5) feedback - user ratings and contact requests. The pgvector extension enables fast cosine similarity search using IVFFLAT indexes optimized for ~10k vectors. All tables have Row Level Security (RLS) policies, with service_role having full access and authenticated users having read/write on their data."
How does the RAG (Retrieval-Augmented Generation) system work?,"The RAG system follows this pipeline: (1) **Query Processing** - User query is classified by type (technical, career, personal) and role context is added (Hiring Manager, Software Developer, etc.), (2) **Embedding Generation** - Query is converted to a 1536-dimensional vector using OpenAI text-embedding-3-small, (3) **Similarity Search** - Supabase pgvector performs cosine similarity search against kb_chunks table, returning top 3 most relevant chunks with scores above 0.7 threshold, (4) **Context Assembly** - Retrieved chunks are formatted with metadata and combined with role-specific prompts, (5) **Response Generation** - OpenAI GPT-4o-mini generates contextual response using retrieved knowledge, (6) **Logging** - Query, response, retrieved chunks, and performance metrics are logged to Supabase for analytics. The system uses LangChain for orchestration and LangSmith for tracing. Average latency: <2 seconds per query."
What files and modules make up the codebase?,"**Core Application**: src/main.py (Streamlit UI entry point), src/agents/role_router.py (role-based routing logic), src/agents/response_formatter.py (output formatting). **RAG Engine**: src/core/rag_engine.py (main RAG orchestration), src/core/response_generator.py (LLM response generation), src/retrieval/pgvector_retriever.py (Supabase vector search). **Configuration**: src/config/supabase_config.py (database connections, API keys), src/config/__init__.py (config exports). **Analytics**: src/analytics/supabase_analytics.py (analytics tracking), src/analytics/data_management/ (backup, migration, quality modules). **External Services**: src/services/storage_service.py (Supabase Storage integration), src/services/ (Resend email, Twilio SMS - planned). **Data**: data/career_kb.csv (knowledge base source), data/code_chunks (code examples for retrieval). **Scripts**: scripts/migrate_data_to_supabase.py (CSV to Supabase with embeddings), scripts/external_services_setup_wizard.py (service configuration helper). **Database**: supabase/migrations/001_initial_schema.sql (database schema), supabase/migrations/CLEAN_MIGRATION.sql (idempotent migration). **Testing**: test_connection.py (API key validation), verify_schema.py (database verification), test_role_functionality.py (comprehensive role tests), test_direct_search.py (vector search tests)."
What are the key features currently implemented?,"**1. Multi-Role Chat Interface**: Dynamically adjusts responses based on selected role (Hiring Manager focuses on career achievements, Software Developer emphasizes technical skills, etc.). **2. Vector Semantic Search**: Uses pgvector for intelligent knowledge retrieval - finds relevant information even with different wording than the knowledge base. **3. Session Management**: Tracks conversation sessions with UUID identifiers, maintains context across multiple queries. **4. Analytics Dashboard**: Logs all interactions with metrics (latency, token usage, success rate), enables data export and performance monitoring. **5. Grounded Responses**: Cites sources from knowledge base, shows which KB chunks were used to generate each answer. **6. Real-time Retrieval Logs**: Displays retrieval quality (similarity scores, which chunks matched) for debugging and optimization. **7. External Links Integration**: Dynamically serves GitHub, LinkedIn, YouTube links based on query context. **8. Feedback System**: Collects user ratings and contact requests, triggers notifications. **9. Error Handling**: Graceful fallbacks when knowledge base lacks information, suggests role switching or provides generic responses. **10. LangSmith Observability**: Full tracing of LLM calls, latency tracking, cost monitoring."
How is the architecture designed for scalability?,"The architecture follows a **serverless-first, stateless design**: **Database Layer**: Supabase provides managed Postgres with automatic backups, read replicas, and connection pooling. pgvector scales horizontally for vector search. **Application Layer**: Stateless Python application designed for Vercel serverless functions - each request is independent, no local state. **Caching Strategy**: Embeddings are pre-computed and stored (not generated on every query), session data persists in database not memory. **API Design**: RESTful endpoints through Supabase PostgREST, RLS policies enforce security at database level. **Observability**: LangSmith provides distributed tracing across serverless invocations. **Cost Optimization**: Uses text-embedding-3-small (cheaper than ada-002), gpt-4o-mini (cheaper than GPT-4), connection pooling reduces database overhead. **Deployment**: Vercel handles auto-scaling, CDN distribution, zero-downtime deployments. **Bottleneck Mitigation**: IVFFLAT indexes reduce vector search from O(n) to O(√n), batched embedding generation reduces API calls. Current capacity: ~10k knowledge base chunks, ~1k concurrent users (with Vercel Pro plan)."
What integrations and external services does the product use?,"**OpenAI API**: GPT-4o-mini for chat completion, text-embedding-3-small for vector embeddings. Used for all LLM and embedding operations. **Supabase**: Postgres database with pgvector, authentication, storage, real-time subscriptions. Hosts all application data and provides REST API. **LangSmith** (optional): LangChain observability platform for tracing LLM calls, debugging prompts, monitoring costs. Set via LANGCHAIN_TRACING_V2 env var. **Resend** (planned): Email delivery API for contact notifications and feedback responses. Will replace previous email integrations. **Twilio** (planned): SMS notifications for urgent contact requests from high-priority users. **Supabase Storage**: File upload/download for future features (resume uploads, document analysis). **Vercel** (deployment): Serverless hosting platform for Next.js frontend and API routes. **GitHub**: Version control, CI/CD via GitHub Actions, code hosting. All API keys stored in .env (local) or Vercel environment variables (production). Services are modular - can be swapped without core architecture changes."
How do I set up and run this product locally?,"**Prerequisites**: Python 3.11+, Git, Supabase account, OpenAI API key. **Setup Steps**: (1) Clone repository: `git clone https://github.com/iNoahCodeGuy/NoahsAIAssistant-.git`, (2) Create virtual environment: `python -m venv .venv` then activate with `.venv\Scripts\Activate.ps1` (Windows) or `source .venv/bin/activate` (Mac/Linux), (3) Install dependencies: `pip install -r requirements.txt`, (4) Configure environment variables in `.env` file: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, (5) Run database migration: Execute `supabase/migrations/CLEAN_MIGRATION.sql` in Supabase SQL Editor, (6) Migrate knowledge base data: `python scripts/migrate_data_to_supabase.py` (generates embeddings and populates database), (7) Verify setup: `python test_connection.py` (validates API keys) and `python verify_schema.py` (checks database schema). **Run Application**: Set PYTHONPATH and start Streamlit: `$env:PYTHONPATH='$PWD\src'; streamlit run src/main.py` (Windows) or `PYTHONPATH=./src streamlit run src/main.py` (Mac/Linux). **Access**: Open http://localhost:8501 in browser. **Troubleshooting**: Check test_connection.py output for API key issues, verify_schema.py for database issues, LangSmith dashboard for LLM tracing."
What is the data flow from user query to response?,"**Complete Request Flow**: (1) **User Input** - User types question in Streamlit UI with role selected (e.g., 'Hiring Manager'), (2) **Session Initialization** - App checks for existing session_id in Streamlit session state, creates new UUID if first query, (3) **Query Classification** - role_router.py analyzes query type (technical, career, personal, product), appends role context to query, (4) **Embedding Generation** - OpenAI text-embedding-3-small converts query to 1536-dimensional vector (~200ms latency), (5) **Vector Search** - pgvector_retriever.py queries Supabase: `SELECT * FROM kb_chunks ORDER BY embedding <=> $query_vector LIMIT 3` with cosine similarity, returns chunks with scores >0.7, (6) **Context Assembly** - Retrieved chunks formatted with section headers, metadata, similarity scores, (7) **Prompt Construction** - response_generator.py builds prompt with system message (role-specific), user query, retrieved context (grounded data), (8) **LLM Call** - OpenAI GPT-4o-mini generates response (~1-2s latency), LangSmith traces call with input/output/latency, (9) **Response Formatting** - response_formatter.py adds citations, formats as markdown, includes source references, (10) **Logging** - supabase_analytics.py logs to messages table (query, answer, role, latency, tokens) and retrieval_logs table (chunk IDs, scores, grounded flag), (11) **UI Update** - Streamlit displays response with expandable sections for sources and system health. **Total latency**: Typically 2-4 seconds end-to-end."
How can developers extend or modify this product?,"**Adding New Features**: (1) **New Role** - Add to role_router.py ROLE_PROMPTS dict with specific instructions, update UI selectbox in main.py, (2) **New Knowledge Base** - Add Q&A pairs to data/career_kb.csv, run migrate_data_to_supabase.py to regenerate embeddings, (3) **New External Service** - Create service class in src/services/, add API keys to .env, integrate in main workflow, (4) **Custom Analytics** - Add columns to messages table via migration, update supabase_analytics.py logging, create views for querying, (5) **UI Customization** - Modify src/main.py Streamlit components, add custom CSS in st.markdown, create new pages in pages/ directory. **Code Architecture**: Follow separation of concerns - agents/ for routing logic, core/ for RAG engine, retrieval/ for data fetching, services/ for external APIs, analytics/ for logging. Use type hints and docstrings. **Testing**: Add tests to tests/ directory, use test_role_functionality.py as template, run with pytest. **Database Changes**: Create new migration SQL in supabase/migrations/, test with verify_schema.py. **Deployment**: Push to GitHub, Vercel auto-deploys from main branch, set environment variables in Vercel dashboard. **Best Practices**: Use environment variables for all secrets, log errors to Supabase, trace LLM calls with LangSmith, use RLS policies for data security, batch operations to reduce API costs."
What are the planned future features and roadmap?,"**Phase 3 (In Progress)**: Next.js frontend migration for better performance and SEO, React components for chat interface, API routes replacing Streamlit. **Phase 4 (Q1 2026)**: Multi-document support (upload resume, cover letter for personalized responses), document parsing with LangChain loaders, hybrid search (keyword + vector). **Phase 5 (Q2 2026)**: Voice interface (speech-to-text input, text-to-speech output), mobile app (React Native), real-time collaboration (multiple users in same session). **Long-term Vision**: AI agent capabilities (autonomous actions, external API calls, calendar integration), fine-tuned model on Noah's communication style, multi-language support, enterprise features (team dashboards, white-labeling, SSO). **Continuous Improvements**: Expand knowledge base with project deep-dives, optimize pgvector index for faster search, implement caching layer (Redis), add A/B testing framework, improve response quality with prompt engineering, add code snippet retrieval with syntax highlighting."
How does the system handle errors and edge cases?,"**Error Handling Strategy**: (1) **No Matching Knowledge** - When similarity scores <0.7, returns 'I don't have enough information' with suggestion to switch roles or rephrase, (2) **API Failures** - Wraps OpenAI calls in try/except, logs errors to Supabase, shows user-friendly message without exposing technical details, retries with exponential backoff, (3) **Database Connection Issues** - Uses connection pooling to prevent exhaustion, gracefully degrades (uses cached responses if available), logs incident to monitoring, (4) **Invalid Input** - Sanitizes user queries (removes SQL injection attempts, limits length to 1000 chars), validates role selection before processing, (5) **Rate Limiting** - Respects OpenAI rate limits (tier-based), implements client-side debouncing on rapid queries, queues requests during high load, (6) **Missing Environment Variables** - test_connection.py validates all required vars on startup, provides clear error messages with setup instructions, prevents app from starting with invalid config, (7) **Embedding Dimension Mismatch** - Validates vector dimensions on insert (must be 1536), migration scripts include validation checks, (8) **Session State Issues** - Initializes session state with defaults, clears corrupted state automatically, provides 'Clear Chat' button for user recovery. **Monitoring**: All errors logged with stack traces, LangSmith alerts on repeated failures, Supabase dashboard shows error rates. **Graceful Degradation**: Returns best-effort responses when services partially fail, never exposes raw exceptions to users."
What security measures are implemented?,"**Authentication & Authorization**: Row Level Security (RLS) policies on all Supabase tables - service_role key for backend operations (full access), authenticated users can only access their own sessions/feedback, anonymous users have read-only on public links. **API Key Security**: All secrets stored in .env (gitignored), never hardcoded in source, Vercel environment variables encrypted at rest, service_role key validated on every database operation. **Input Validation**: User queries sanitized before embedding/storage, SQL injection prevention via parameterized queries (PostgREST), length limits on all text inputs (1000 chars for queries, 5000 for feedback). **Data Privacy**: Messages table logs minimal PII (no names/emails unless user provides in feedback), session_id is UUID not linked to identity, option to disable analytics logging via config. **External API Security**: HTTPS for all external calls (OpenAI, Supabase, Twilio, Resend), API keys rotated periodically, rate limiting to prevent abuse, request signing for webhook verification. **Database Security**: Postgres connection over SSL, password authentication required, IP whitelist for direct database access, backup encryption enabled. **Frontend Security**: Streamlit XSRF protection enabled, no eval() or exec() of user input, markdown sanitization to prevent XSS, iframe sandboxing for external content. **Monitoring**: Failed authentication attempts logged, unusual query patterns flagged, LangSmith tracks API key usage for anomaly detection. **Compliance**: GDPR-friendly (data export/deletion features), no third-party analytics cookies, user feedback clearly marked as stored."
How does this chatbot product work?,"This AI chatbot works by combining **retrieval-augmented generation (RAG)** with **role-aware context**. When you ask a question, the system: (1) Classifies your query type (technical, career, personal), (2) Generates a semantic embedding of your question using OpenAI, (3) Searches Noah's knowledge base (stored in Supabase pgvector) for relevant information, (4) Retrieves the top 3-5 most similar chunks with context about Noah, (5) Feeds this context to GPT-4o-mini which generates a personalized response about Noah, (6) Adds role-specific enhancements (code snippets for developers, career highlights for hiring managers), (7) Logs the interaction for analytics and improvement. The backend is **Python + LangGraph** orchestrating the conversation flow, **Supabase** for database/vector storage, **OpenAI** for LLM and embeddings, and **Next.js** for the frontend UI. Everything runs as **Vercel serverless functions** for scalability. The key innovation is the **role router** that adapts responses based on who you are (hiring manager sees different content than a software developer)."
What backend technologies power this assistant?,"The backend stack is: **Python 3.11+** for core logic and orchestration, **LangGraph** for multi-step conversation flows (classify → retrieve → generate → execute → log pipeline), **LangChain** for LLM abstraction and prompt management, **OpenAI GPT-4o-mini** for natural language generation, **OpenAI text-embedding-3-small** for 1536-dimensional vector embeddings, **Supabase Postgres** with **pgvector extension** for vector similarity search and relational data storage, **Supabase Storage** for file management (resume PDFs), **Vercel serverless functions** for API deployment (auto-scaling, zero-downtime), **LangSmith** for observability (tracing LLM calls, cost monitoring), **Resend** for transactional emails, **Twilio** for SMS notifications. The architecture is **stateless** - each request is independent, with session state stored in Supabase not memory. This enables horizontal scaling and serverless deployment."
How was this AI assistant built?,"Noah built this AI assistant through iterative development: **Phase 1 (Foundation)**: Started with a simple Streamlit app and FAISS local vector store for knowledge retrieval. Built the RAG pipeline using LangChain to combine OpenAI embeddings with retrieval. **Phase 2 (Role Intelligence)**: Added the role router system to personalize responses for different user types (hiring managers, developers, casual visitors). Implemented session management and analytics logging. **Phase 3 (Production Infrastructure)**: Migrated from FAISS to Supabase pgvector for cloud-native vector search. Added LangGraph for workflow orchestration (replaced linear pipeline with node-based flow). Built Next.js frontend to replace Streamlit for better performance. Deployed to Vercel serverless for auto-scaling. **Phase 4 (Refinements)**: Implemented third-person language enforcement (responses say 'Noah' not 'I'). Added intelligent follow-up question suggestions for technical roles. Expanded knowledge base from career-only to include technical_kb (system details) and architecture_kb (diagrams, code examples). The development process emphasized **rapid prototyping** (get it working first), **observability** (LangSmith tracing for debugging), and **incremental complexity** (add features one at a time, validate each works)."
How does this product work?,"This product is an AI-powered interactive resume assistant built by Noah. It works by: (1) You ask a question about Noah (career, skills, projects), (2) The system converts your question into a semantic vector embedding, (3) Searches Noah knowledge base in Supabase pgvector for relevant information, (4) Retrieves the top matching chunks about Noah, (5) Feeds context to OpenAI GPT-4o-mini, (6) Generates a personalized response about Noah in third-person, (7) Adds role-specific enhancements (code for developers, achievements for hiring managers). The **backend** uses Python + LangGraph for orchestration, Supabase for database/vectors, OpenAI for LLM, and deploys as Vercel serverless functions. The **frontend** is Next.js with Tailwind CSS. The system is **role-aware** - responses adapt based on whether you are a hiring manager, software developer, or casual visitor."
