# Noah's AI Assistant

Noah's AI Assistant is an **educational generative AI application** that teaches users how GenAI systems work and their enterprise value by **using itself as a hands-on case study**. 

This is a **complete full-stack AI system** demonstrating every component enterprises need: **frontend** (UI/UX), **backend** (APIs, business logic), **data pipelines** (ETL, embeddings), **architecture** (RAG, vector search), **QA** (testing strategies), and **DevOps** (deployment, monitoring). Through interactive conversation, it explains its own implementation and shows how each pattern applies to enterprise use cases like customer support, internal documentation, and sales enablement.

**üéì The Learning Approach**: Instead of abstract explanations, the assistant shows you the actual code, architecture diagrams, and data flows that power the conversation you're having. 

**üìò [Start Here: Complete System Learning Guide](docs/LEARNING_GUIDE_COMPLETE_SYSTEM.md)** - Comprehensive walkthrough of every component with enterprise mapping.

**Quick explorations:**
- "Show me the backend stack" ‚Üí See real Python code with annotations
- "How does RAG work?" ‚Üí Walk through the retrieval pipeline with examples
- "Explain the data pipeline" ‚Üí Document processing and embedding generation
- "Show me the frontend code" ‚Üí UI patterns and session management
- "How do you test everything?" ‚Üí QA strategies and mocking patterns
- "Display data analytics" ‚Üí View live metrics and understand observability
- "What makes this valuable to enterprises?" ‚Üí Connect technical patterns to business ROI

The assistant adapts its teaching style based on user roles (technical vs. non-technical), making complex AI concepts accessible while showcasing production-ready implementation patterns across the entire stack.

## Table of Contents
- [üöÄ Quickstart](#-quickstart)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Role-Specific Behaviors](#role-specific-behaviors)
- [Installation](#installation)
- [Usage](#usage)
- [File Structure](#file-structure)
- [Contributing](#contributing)
- [License](#license)

## üöÄ Quickstart

**Get Noah's AI Assistant running locally in 5 minutes**

### Prerequisites
- ‚úÖ Python 3.11+ installed
- ‚úÖ OpenAI API key ([get one here](https://platform.openai.com/api-keys)) - $5 free credit for new accounts
- ‚úÖ Supabase account ([sign up free](https://supabase.com/dashboard)) - Free tier is sufficient

### Quick Setup

```bash
# 1. Clone the repository
git clone https://github.com/noahcal/noahs-ai-assistant.git
cd noahs-ai-assistant

# 2. Install dependencies
pip install -r requirements.txt

# 3. Set up environment variables
# Create a .env file with your API keys:
OPENAI_API_KEY=sk-...
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_SERVICE_KEY=eyJhbGc...
```

**Get Supabase credentials**:
1. Create a new project in [Supabase Dashboard](https://supabase.com/dashboard)
2. Go to Settings ‚Üí API
3. Copy `URL` and `service_role` key (‚ö†Ô∏è keep this secret!)

```bash
# 4. Set up the database schema
# Follow the step-by-step guide in docs/PHASE_1_SETUP.md
# This creates tables for knowledge base, messages, and analytics

# 5. Run the data migration (one-time)
python scripts/migrate_data_to_supabase.py

# 6. Start the Streamlit app
streamlit run src/main.py
```

**üéâ Success!** Open http://localhost:8501 in your browser.

**First steps**:
1. Select a role (try "Software Developer" to explore technical architecture)
2. Ask questions like:
   - "How does RAG work in this application?"
   - "Show me the backend architecture"
   - "Explain the data pipeline"
   - "What makes this valuable to enterprises?"
3. See the assistant explain its own implementation with code examples!

**Need help?** See [PHASE_1_SETUP.md](docs/PHASE_1_SETUP.md) for detailed setup instructions.

**Next**: Learn about the [Complete System](docs/LEARNING_GUIDE_COMPLETE_SYSTEM.md) covering frontend, backend, data pipelines, architecture, QA, and DevOps.

## üìö Complete System Components

This assistant demonstrates **every layer of a production AI application**:

### üé® Frontend Development
- **UI/UX Patterns**: Chat interface, role selection, session management
- **Professional Rendering**: Markdown tables, data visualization, action buttons
- **State Management**: UUID-based sessions, conversation history, error boundaries
- **Enterprise Application**: Maps to customer portals, internal tools, CRM widgets
- **Ask:** "Show me your frontend code" | "How does the chat interface work?"

### ‚öôÔ∏è Backend Architecture
- **API Design**: Serverless routes (`/api/chat`, `/api/analytics`, `/api/email`)
- **Business Logic**: LangGraph orchestration, service layer, state management
- **Error Handling**: Graceful degradation, retry logic, timeout management
- **Enterprise Application**: Same patterns for support bots, doc assistants, sales tools
- **Ask:** "Show me the backend stack" | "Explain the conversation pipeline"

### üìä Data Pipeline Management
- **ETL Process**: Document ingestion, parsing, chunking (500-token strategy)
- **Embedding Generation**: OpenAI text-embedding-3-small (768 dimensions, $0.0001/1K tokens)
- **Storage**: Supabase Postgres + pgvector, idempotent migrations
- **Enterprise Application**: Scales to Confluence, Zendesk, SharePoint ingestion
- **Ask:** "Show me the data pipeline" | "How do you process documents?"

### üèóÔ∏è System Architecture
- **RAG Pipeline**: Query embedding ‚Üí Vector search ‚Üí Context assembly ‚Üí LLM generation
- **Vector Search**: pgvector with IVFFLAT index, cosine similarity, top-k retrieval
- **LLM Orchestration**: GPT-4o-mini with temperature control, token optimization
- **Enterprise Application**: Proven patterns for accuracy, cost, and maintainability
- **Ask:** "How does RAG work?" | "Show me vector search code"

### üß™ QA & Testing
- **Test Strategy**: Pytest framework, unit + integration tests, mocking patterns
- **Edge Cases**: Empty queries, malformed input, XSS attempts, concurrent sessions
- **Quality Gates**: Linting, type checking, coverage thresholds
- **Enterprise Application**: Same patterns + domain-specific tests (PII, compliance)
- **Ask:** "Show me your testing strategy" | "How do you mock Supabase?"

### üöÄ DevOps & Deployment
- **Infrastructure**: Vercel serverless (auto-scaling), Supabase (managed DB), LangSmith (monitoring)
- **CI/CD**: Git push ‚Üí automated tests ‚Üí zero-downtime deployment
- **Cost Tracking**: $25/month for production system, scales to $3200/month for 100k users
- **Enterprise Application**: Kubernetes, Redis caching, enterprise SLA, security hardening
- **Ask:** "What's your deployment process?" | "Show me cost at 100k users"

**‚Üí [Complete System Learning Guide](docs/LEARNING_GUIDE_COMPLETE_SYSTEM.md)** - Deep dive into each component with enterprise use case mapping.

---

## What You'll Learn

Through interactive conversation, this assistant teaches you:

### For Technical Audiences (Full-Stack Developers, DevOps Engineers)
- **Frontend Patterns**: UI components, session management, error boundaries, professional data rendering
- **Backend Design**: API routes, service layer, state management, LangGraph orchestration
- **Data Pipelines**: ETL processes, chunking strategies, embedding generation, incremental updates
- **RAG Architecture**: How Retrieval-Augmented Generation works (vector embeddings, semantic search, context injection)
- **Vector Search**: Supabase pgvector implementation with IVFFLAT indexing and similarity scoring
- **LLM Orchestration**: Node-based conversation flows with temperature control and token optimization
- **QA Strategies**: Unit testing, integration testing, mocking external services, edge case validation
- **DevOps Practices**: Serverless deployment, environment management, CI/CD, cost optimization
- **System Design**: Hybrid deployment (Streamlit + Vercel), monitoring, observability, scaling patterns
- **Production Patterns**: Analytics logging, PII redaction, rate limiting, security (RLS policies)
- **Cost Optimization**: Model selection, caching strategies, and infrastructure choices
- **Testing & Reliability**: Pytest patterns, Supabase mocking, error degradation modes

### For Non-Technical Audiences
- **GenAI Business Value**: Why enterprises invest in AI assistants (cost savings, scalability, 24/7 availability)
- **Accuracy & Trust**: How RAG prevents hallucinations by grounding responses in real data
- **Data Governance**: PII protection, privacy controls, compliance considerations
- **ROI & Metrics**: Success measurement (response time, accuracy, user satisfaction, conversion rates)
- **Competitive Advantage**: Using AI to improve customer experience and operational efficiency
- **Implementation Roadmap**: High-level steps to build similar systems for your organization

### Live Demonstrations
The assistant demonstrates concepts by showing its own implementation:
- **Ask "show me the backend stack"** ‚Üí See real Python code with explanations
- **Ask "display data analytics"** ‚Üí Live dashboard with real-time metrics
- **Ask "how do you prevent hallucinations?"** ‚Üí RAG pipeline walkthrough with examples
- **Ask "what's your data pipeline?"** ‚Üí Document processing flow with architecture diagrams

## Features

### üéì Educational Features (Learn GenAI Through Demonstration)
- **Self-Referential Teaching**: Explains RAG, vector search, and LLM orchestration by showing its own implementation
- **Live Code Walkthroughs**: Displays actual Python files with explanations of GenAI patterns and enterprise applications
- **Architecture Tours**: Interactive exploration of stack, data pipelines, backend design, and deployment strategies
- **Enterprise Value Mapping**: Connects every technical concept to business ROI (cost reduction, scalability, reliability)
- **Role-Adaptive Teaching**: Adjusts depth for technical (with code) vs. non-technical (with analogies) audiences
- **Real Metrics Dashboard**: Shows live analytics to explain observability, logging, and data governance in production

### üèóÔ∏è Technical Features (Production-Ready Patterns)
- **Retrieval-Augmented Generation (RAG)**: Supabase pgvector with IVFFLAT indexing for semantic search
- **LangGraph Orchestration**: Node-based conversation flows with immutable state management
- **Dual-Audience Formatting**: Technical details + plain-English summaries for mixed audiences
- **Code & Career Grounding**: File:line citations proving all responses are grounded in real data
- **PII Redaction**: Automated masking of emails/phones in user feedback (demonstrates data governance)
- **Rate Limiting**: 6 req/min per IP with graceful degradation (demonstrates production reliability)
- **Live Analytics Dashboard**: Real-time data visualization with PII redaction and role-based access
- **MMA Query Routing**: Direct fight link for MMA-related queries (demonstrates multi-modal routing)
- **Confession Mode**: Privacy-protected personal messaging (demonstrates data governance)
- **Analytics Tracking**: Interaction logging, retrieval tracking, user feedback with Supabase
- **Contact Requests**: Email delivery via Resend, SMS notifications via Twilio
- **Extensible Orchestration**: LangGraph-style node-based conversation flow
- **Observability**: LangSmith integration for tracing and performance monitoring

## Tech Stack

### Current Architecture (Supabase + Vercel)
- **Frontend/UI**: Streamlit (chat UI, role selector, session management)
- **Core Framework**: LangChain (loaders, embeddings, retrieval pipeline)
- **Database**: Supabase Postgres with pgvector extension
- **Vector Search**: pgvector with IVFFLAT indexing for semantic similarity
- **Models**: OpenAI GPT-3.5/4 (generation), OpenAI ada-002 (embeddings)
- **Analytics**: Direct Supabase writes (messages, retrieval_logs, feedback)
- **External Services**: 
  - Resend (email delivery)
  - Twilio (SMS notifications)
- **API Layer**: Next.js API routes for external integrations
- **Security**: Row Level Security (RLS) policies, environment variables
- **Deployment**: Hybrid (Streamlit + Vercel serverless)
- **Observability**: LangSmith integration for tracing
- **Testing**: Pytest with Supabase mocking

### Cost Estimates
- **Supabase Pro**: ~$25/month (includes PostgreSQL + pgvector + Storage)
- **OpenAI API**: Variable based on usage (~$10-30/month for moderate traffic)
- **Vercel**: Free tier (serverless functions for API routes)
- **Resend**: Free tier up to 3,000 emails/month
- **Twilio**: Pay-as-you-go (~$0.0075/SMS)
- **Total**: ~$35-60/month for production deployment

**Previous GCP Architecture** (archived): Cost ~$100-200/month
- Used Cloud SQL, Pub/Sub, Secret Manager, Vertex AI, Cloud Run
- Migrated to Supabase for 50-75% cost reduction and simplified maintenance

## Role-Specific Behaviors

### 1. Hiring Manager (nontechnical)
- Goal: High-level r√©sum√© narrative
- Sources: Career KB CSV, r√©sum√© text
- Format: Career Overview ‚Üí Notable Outcomes ‚Üí Source Citations (lightweight)
- UI: Contact CTA (email / LinkedIn)

### 2. Hiring Manager (technical)
- Goal: Blend career signal + engineering depth
- Sources: Career KB + Code Index
- Format:
  - Section 1: Engineer Detail (with file:line citations)
  - Section 2: Plain-English Summary
- UI: Expandable code reference (future enhancement)

### 3. Software Developer
- Goal: Deep technical explanation, architecture, design tradeoffs
- Sources: Code Index + Career KB fallback
- Format: Engineer Detail ‚Üí Summary (same dual format)

### 4. Just Looking Around
- Goal: Fun facts + MMA info
- MMA queries: Direct fight link
- Other queries: Light career / personal facts

### 5. Looking to Confess Crush
- Goal: Safe, minimal interaction channel
- Storage: Only explicit submission, no hidden PII retention
- Response: Acknowledgment only (no model overreach)

## Installation

### Prerequisites
- Python 3.10+
- Supabase account (free tier available)
- OpenAI API key
- (Optional) Resend API key for email
- (Optional) Twilio credentials for SMS

### Setup Steps

1. **Clone the repository**
```bash
git clone https://github.com/iNoahCodeGuy/NoahsAIAssistant-.git
cd NoahsAIAssistant-
```

2. **Install Python dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure Supabase**
   - Create a new project at [supabase.com](https://supabase.com)
   - Run the SQL migration in `supabase/migrations/001_initial_schema.sql` in your Supabase SQL Editor
   - Copy your project URL and service role key

4. **Set environment variables**
```bash
cp .env.example .env
# Edit .env and add:
# SUPABASE_URL=your_project_url
# SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
# OPENAI_API_KEY=your_openai_key
# (Optional) RESEND_API_KEY, TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN, TWILIO_FROM
```

5. **Populate knowledge base** (optional)
```bash
python scripts/migrate_data_to_supabase.py
```

## Usage

### Development
Run the Streamlit application locally:
```bash
streamlit run src/main.py
```

### Production Deployment

**Option 1: Streamlit Community Cloud**
- Deploy directly from GitHub
- Set environment variables in Streamlit Cloud dashboard
- Best for chat interface only

**Option 2: Hybrid (Streamlit + Vercel)**
- Deploy Streamlit for chat UI
- Deploy Next.js API routes to Vercel for email/SMS integrations
- Configure `vercel.json` for serverless functions

### Database Schema
The Supabase database includes:
- `kb_chunks`: Knowledge base with pgvector embeddings
- `messages`: Chat interaction logs
- `retrieval_logs`: RAG pipeline tracking
- `links`: External resource URLs
- `feedback`: User ratings and contact requests

### Architecture Overview
```
User ‚Üí Streamlit UI ‚Üí RagEngine (pgvector search) ‚Üí OpenAI GPT ‚Üí Response
                ‚Üì
        Supabase Analytics (messages, retrieval_logs)
                ‚Üì
        Feedback ‚Üí Email (Resend) / SMS (Twilio)
```

## File Structure
```
noahs-ai-assistant
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ ui/
‚îÇ   ‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ vector_stores/
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## Contributing
1. Create a branch: feature/<name>
2. Keep commits focused
3. Open PR ‚Üí request review

## License
MIT (see LICENSE if present). Add LICENSE file if not yet created.
